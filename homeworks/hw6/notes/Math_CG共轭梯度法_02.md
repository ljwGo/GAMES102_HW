[toc]

## 3.2 Krylov子空间

​	**它是一个通过将一个矩阵反复左乘一个向量构建起来的子空间**.

​	因为$r_i$和$d_j$彼此互为线性相关的复杂关系, 添加新维度时, 视为在原有空间基础上左乘A.
$$
D_1 = \{d_0\} \\
D_2 = D_1 + AD_1 = \{d_0, Ad_0\} \\
D_{i+1} = D_i + AD_i \\
= (D_{i-1} + AD_{i-1}) + (AD_{i-1} + A^2D_{i-1}) \\
... 约去常数系数(向量空间与长度无关)\\
= D_1 + AD_1 + ... + A^iD_1 \\
= \{{d_0, Ad_0, A^2d_0, ..., A^id_0}\}
$$
​	同理, 可以求出
$$
D_{i+1} = \{{r_0, Ar_0, A^2r_0, ..., A^ir_0}\}
$$
​	由2.3.5得$此时u_i = r_i$
$$
{r_i}^Tr_i = {r_i}^Td_i \\ 
{r_k}^Tr_i = 0,& k\neq i时
\tag{3.2.1}
$$
​	$由a_i = \frac{{d_i}^Tr_i}{{d_i}^TAd_i}$和3.2.1得
$$
由a_i = \frac{{r_i}^Tr_i}{{d_i}^TAd_i} \tag{3.2.2}
$$
由$r_{i+1} = r_i - a_iAd_i$, 两边左乘$r_k$
$$
r_kr_{i+1} = r_kr_i - a_ir_kAd_i \\
r_kAd_i = \frac{r_kr_i - r_kr_{i+1}}{a_i} \\
\\
由 4.0.1\\
\begin{cases}
r_iAd_i = \frac{r_ir_i}{a_i}, & 当k=i时 \\
r_{i+1}Ad_i = \frac{- r_{i+1}r_{i+1}}{a_i}, & 当k=i+1时\\
0, & else
\end{cases}
$$
带入3.2.2得到结果
$$
\begin{cases}
r_iAd_i = {d_i}^TAd_i, & 当k=i时 \\
r_{i+1}Ad_i = \frac{- {r_{i+1}}^Tr_{i+1}}{{r_i}^Tr_i}{d_i}^TAd_i, & 当k=i+1时\\
0, & else
\end{cases}
$$
**联系2.0.1得**
$$
u_{ij} = \frac{{v_i}^TAd_j}{{d_j}^TAd_j} & (i>j)\\
= \frac{{r_i}^TAd_j}{{d_j}^TAd_j} \\
=
\begin{cases}
\frac{- r_{i}r_{i}}{r_{i-1}r_{i-1}}, & i = j+1 \\
0, & i>j+1
\end{cases}
\tag{3.2.3}
$$
得到重要的公式3.2.3, 在获取新的方向时, 不需要使用前面所有的方向进行计算了, 只需要**保存上一次的残差向量即可**



## 3.3 共轭梯度法相关公式

$$
1. d_0 = r_0 \\
2. x_{i+1} = x_i + a_id_i \\
3. d_i = r_i - u_{ij}d_{i-1} & i=j+1 \\
4. u_{ij} = \frac{r_{i}r_{i}}{r_{i-1}r_{i-1}}, & i = j+1 \\
5. r_{i+1} = r_i - a_iAd_i \\
6. a_i = \frac{{r_i}^Tr_i}{{d_i}^TAd_i}
$$



# 4. 共轭梯度法的收敛性

## 4.1 收敛性

原文对共轭梯度法的收敛性证明相当复杂.

结论:
$$
||e_i||_A \leq 2(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^i||e_0||_A
$$
可见, 矩阵的条件范数影响CG收敛速度.



## 4.2 复杂度

时间复杂度，**SG**算法为$O(mκ)$，而**CG**算法为$O(m\sqrt{\kappa})$;

空间复杂度，均为$O(m)$.



# 5. 标准方程的共轭梯度

​		CG算法还可以用来解决A非对称, 非正定, 甚至非方阵的问题. 对给定的最小二乘问题, 如下所示:
$$
min||Ax-b||^2
$$
  其解可以通过令上式的导出式为0求得,  如下所示:
$$
A^TAx = A^Tb
$$
  若A为非奇异的方阵, 上式的解即为$Ax=b$的解. 若A为非方阵, 则$Ax=b$过约束(线性无关的方程数量超过了自变量数量), 那么$Ax=b$必定无解, 但我们总有可能找到一个向量$x$使得等式也即每个线性方程的误差的平方和最小化.

  $A^TA$是对称并且正定的(对任意给定的向量$x$, 均$x^TA^TAx=||Ax||^2 \gt 0$. 若$Ax=b$并非欠约束的, 则$A^TA$就是非奇异的, 我们就可以使用类似SG以及CG算法来求解式. 唯一比较讨厌的一点就是, $A^TA$的条件数是A条件数的平方, 因此其收敛速度会明显慢很多.

  一个重要的技术点是, 我们并不会显式的去构建矩阵$A^TA$, 因为它远比A稀疏. 取而代之的是, 先用$A$乘$d$求出乘积$Ad$, 再求出$A^TAd$. (这里结合共轭梯度法中所有可用公式好理解些)



## 5.1 预处理

​	预处理是一项改善矩阵条件数的技术. 假定矩阵**M**为一更方便求解逆矩阵的近似矩阵**A**的对称正定阵. 我们可以通过间接的求解下式来求解$Ax=b$:
$$
M^{-1}Ax = M^{-1}b
$$
  若$\kappa(M^{-1}A) < \kappa(A)$, 或者说$M^{-1}A$的特征值分布比$A$集中, 我们递归求解上式要远比求解原始问题快. 问题在于$M^{-1}A$通常并非对称或者正定的, 即使M和A都是.

  我们可以绕过这个棘手的问题, 因为对于任意对称正定阵**M**, 必然存在矩阵(并不一定唯一)**E**满足$EE^T=M$(像这样的矩阵**E**可以通过诸如[Cholesky factorizationi/decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition)方法获得). 矩阵$M^{-1}A$和$E^{-1}AE^{-T}$具有相同的特征值. 该结论之所以成立, 是因为若$v$为矩阵$M^{-1}A$的一个特征值为$\lambda$的特征向量, 那么$E^Tv$则为矩阵$E^{-1}AE^{-T}$特征值为$\lambda$的特征向量.

  由此, 线性方程组$Ax=b$可变换为如下问题:
$$
E^{-1}AE^{-T}\overline{x} = E^{-1}b, \overline{x}=E^Tx
$$
  针对上述方程我们先求$\overline{x}$, 再求解$x$. 由于$E^{-1}AE^{-T}$是对称正定阵, 可通过**SG**或**CG**算法解出.



# 6. 其它

​	比如非线性的CG算法参考原文吧.
