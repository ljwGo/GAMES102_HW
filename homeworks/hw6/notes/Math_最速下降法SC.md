[toc]

# 0. 序言

​	求解稀疏矩阵的方法多是迭代法, 比如最速下降法, 雅各比分解法, 共轭梯度法等. 稀疏矩阵是指零元素占比巨大的矩阵.

## 0.1 参考

[共轭梯度法通俗讲义 | 断鸿声里，立尽斜阳 (flat2010.github.io)](https://flat2010.github.io/2018/10/26/共轭梯度法通俗讲义/)

这个是翻译的国外的, 写得很详细, 通俗易懂.



# 1. 二次型

​	二次型的形式和经典的二次函数相似
$$
二次函数: f=ax^2 - bx + c \\
二次型: f=\frac{1}{2}x^TAx - b^Tx + \overline{c}  & x是向量
$$
二次型的导数(根据矩阵导数公式)
$$
f\prime(x) = \frac{1}{2}Ax + \frac{1}{2}A^Tx - b
$$
如果A是实对称矩阵(实指的是特征值都是实数,不存在复数)
$$
A = A^T \tag{1.1}
$$
* 式子简化为

$$
f\prime(x) = Ax - b
$$
可以证明(1.4)当A是正定矩阵时, 二次型表示的曲面是**开口向上的抛物曲面, 有唯一最小的x解, 该解是Ax=b的解, 此时$f\prime(x)$=0, 是驻点**

* 在最小值点x处

$$
b = Ax \tag{1.2}
$$

为了证明还要用到下面得定理
$$
若A^T = B, A^T = A, 则A=B. \tag{1.3}
$$
证明
$$
设p是二次型曲面的任意点, 而x是最小值点(p\neq x), 做f(p)-f(x) \\
f(p)-f(x) = \frac{1}{2}p^TAp - b^Tp - \frac{1}{2}x^TAx + b^Tx \\
= \frac{1}{2}p^TAp - (Ax)^Tp - \frac{1}{2}x^TAx + (Ax)^Tx \\
= \frac{1}{2}p^TAp - x^TAp - \frac{1}{2}x^TAx + x^TAx & 由1.1得\\
= \frac{1}{2}p^TAp - x^TAp + \frac{1}{2}x^T A x \\
= \frac{1}{2}p^TAp - \frac{1}{2}x^TAp - \frac{1}{2}x^TAp + \frac{1}{2}x^T A x
$$
证明第二部分
$$
x^TAp结果是一个1*1矩阵m, 其转置还是自身m. \\
x^TAp转置表达式上是p^TAx. \\
由1.3得x^TAp = p^TAx \tag{1.4}
$$
证明第三部分
$$
\frac{1}{2}p^TAp - \frac{1}{2}x^TAp - \frac{1}{2}x^TAp + \frac{1}{2}x^T A x \\
= \frac{1}{2}p^TAp - \frac{1}{2}x^TAp - \frac{1}{2}p^TAx + \frac{1}{2}x^T A x \\
= \frac{1}{2}p^TA(p-x) + \frac{1}{2}x^TA(x-p) \\
= \frac{1}{2}(p^T-x^T)A(p-x) \\
= \frac{1}{2}(p-x)^TA(p-x) \tag{1.5}
$$
由于A是正定矩阵, 所以$(p-x)^TA(p-x)>0$. 因此f(p)-f(x)>0. f(x)是最小值点. 证毕.



设$f\prime(x)=0$, 此时表示二次型曲面的最小值. 并且有Ax=b. 这样就将求解方程组的问题转化为求解二次型最小值的问题.



# 2. 最速下降法(梯度下降法)

​	梯度下降法可以说是求解最小值的常客了. 这里不多说.

当A是实对称正定矩阵. 二次型有唯一最小解.
$$
f\prime(x) =
\left[
\begin{matrix}
\frac{\part{f}}{x_1} \\ 
\frac{\part{f}}{x_2} \\ 
\frac{\part{f}}{x_3} \\ 
... \\
\frac{\part{f}}{x_n} \\
\end{matrix}
\right]
$$

## 2.1 误差向量和残差向量

* 误差向量$\overline{e_i}=x_i - x(x是最小解)$

* 残差向量$\overline{r_i}=b-Ax_i$

易得
$$
\overline{r_i}=-f\prime{(x_i)} \\
梯度的负方向;
$$
另外
$$
\overline{r_i}=-A\overline{e_i} \tag{2.1}
$$
证明如下: 
$$
\overline{r_i}=b-A(\overline{e_i} + x) \\
=b-A\overline{e_i} - Ax \\
=b-A\overline{e_i} - b \\
=-A\overline{e_i}
$$

## 2.2 更新公式

* 第i步的公式为:

$$
x_{i+1} = x_i + a_i\overline{r_i} \tag{2.2}
$$

在机器学习中, $a$是人工指定的, 收敛点是尝试出来的. 然而上式步长可以求解出来.

* **步长求解**(文章中有图, 看图后很好理解)

  要求最小值, 这里可以使用导数为0, 来求驻点.

  ps:
  
  * 注意二次型为标量函数
  * 方向导数是**标量函数对标量(在方向上移动的距离)**的求导, 方向是隐含的
  * 方向导数结果是标量, 为梯度和方向向量的点乘(联想偏导数)
  
  
  
  **二次型方向导数**为
  $$
  \frac{d}{da}f(x_{i+1})
  $$
  利用求导链式法则
  $$
  \frac{d}{da}f(x_{i+1}) = (\frac{df(x_{i+1})}{dx_i})^T \cdot \frac{dx_i}{da} \\
  = -(r_{i+1})^T \cdot r_i \\
  r_i由2.2得到 \\
  r_{i+1}由梯度反方向定义得到
  $$
  接下来求解$a$
  $$
  {r_{i+1}}^T \cdot r_i = (b-Ax_{i+1})^T \cdot r_i \\
  = (b-A(x_i+a_ir_i))^T \cdot r_i \\
  = (b-Ax_i-a_iAr_i)^T \cdot r_i \\
  = (b-Ax_i)^T \cdot r_i - a_i(Ar_i)^T \cdot r_i \\
  = {r_i}^Tr_i - a_i{r_i}^TAr_i
  $$
  
  令方向导数为0, 则
  $$
  a_i = \frac{{r_i}^Tr_i}{{r_i}^TAr_i}
  $$
  
* 结果简化

  对2.2进行处理, 先是两边同时乘上-A, 再分别加上b. 最总得到

$$
\begin{cases}
a_i = \frac{{r_i}^Tr_i}{{r_i}^TAr_i} \\
r_{i+1} = r_i - a_i A r_i \tag{2.3}
\end{cases}
$$

​	对于每次迭代, 只需要计算一次矩阵\*向量乘法$a_i A \overline{r_i}$即可. 现在迭代的是残差向量. 计算完全不依赖于$\overline{x_i}$，这样由于迭代过程中浮点舍入误差的累积，最终只会收敛到最小值$\overline{x}$的附近，而不是最小值点本身。要避免这个问题也非常简单，定期使用$$\overline{x_i}$$计算残差向量$$\overline{r_i}$$即可.



# 3. 特征值回顾和Jacobi迭代

## 3.0 实对称阵必然存在n个线性无关的特征向量, 并且它们正交

​	假设A为任意n阶方阵, 其拥有n个线性独立的特征向量.

将任意其它向量x看作是这n个线性独立特征向量($v_1, v_2, ..., v_n$)的线性组合.

比如:
$$
x = k_1 v_1 + k_2 v_2
$$
则
$$
A^ix = A^ik_1v_1 + A^ik_2v_2 & (常量提出来)\\
=k_1 A^i v_1 + k_2 A^i v_2 \\
=k_1 {\lambda}_1^i v_1 + k_2 {\lambda}_2^i v_2
$$
当所有特征值${\lambda_i}$绝对值均小于1, $A^ix$趋近于**0**. 且值越小, 越快能趋近于**0**.



*摘自原文*

>​	实际上对大多数非实对称阵而言，上述结论也成立（存在n个线性无关的特征向量）。但是这里还有必要提一下的是，有一类非实对称阵，它们不具备n个线性独立的特征向量，这类矩阵我们称之为[退化矩阵(defective)](https://en.wikipedia.org/wiki/Defective_matrix)。光从这个名字你就可以看出那些因研究该类矩阵而受挫的线性代数学家们对它当之无愧的抵触。
>
> 关于这类矩阵的细节问题由于太过复杂因此无法在本文中详述，请自行参考相关资料。但是这类矩阵的特性可以通过[广义特征向量(generalized eigenvector)](https://en.wikipedia.org/wiki/Generalized_eigenvector)和广义特征值(generalized eigenvalue)来分析。对于退化矩阵，当且仅当其所有广义特征值的绝对值均小于1时，$B^ix$方能收敛到0。要证明这点非常难，感兴趣可参考相关资料。



## 3.1 正定矩阵特征值大于0

证明: (v为特征向量)
$$
v^TAv = v^T\lambda v = \lambda v^Tv \\
因为v^Tv > 0 \\ 
所以\lambda > 0
$$


## 3.2 Jacobi迭代

​	Jacobi是一种迭代求解Ax=b的方法.

* 将A分解为D和E, 其中D是对角线矩阵(对角线上元素非零, 其他位置为零); E是对角线元素为零, 其它元素非零的矩阵

  易得
  $$
  A = D + E
  $$

* 

* 求解过程
  $$
  Ax = b \\
  (D+E)x = b \\
  D^{-1}(D+E)x = D^{-1}b \\
  x = - D^{-1}Ex + D^{-1}b
  $$
  令$-D^{-1}Ex为B, D^{-1}b为z$, 则
  $$
  x = Bx + z \tag{3.2}
  $$
  *ps:*

  >该式只有在最小值x点才严格成立

* 利用3.2得到迭代的公式
  $$
  x_{i+1} = Bx_i + z \tag{3.2.1}
  $$
  *ps:*

  > 这里的等号是赋值操作, 迭代收敛的条件是经过一次迭代后, $x_{i+1} = x_i$, 也就是达到稳定状态



* 3.2.1的变式(x是最小值向量)
  $$
  e_{i+1} + x = B(e_i + x) + z \\
  e_{i+1} + x = Be_i + Bx + z \\
  e_{i+1} + x = Be_i + x & 由3.2得 \\
  e_{i+1} = Be_i \tag{3.2.2}
  $$
  当$\rho(B) < 1$时,e必然收敛, 则迭代结果必然趋近于最小值点x.

  

*摘自原文*

>​	根据上式可以这样来理解迭代过程：每一轮迭代时，不会影响迭代值$x_i$对应于真实解的部分，而只是影响误差项
>
>​	由于矩阵B并非总是对称阵（即使矩阵A是对称阵），甚至可能是退化矩阵。而雅可比算法的收敛速度又很大程度上依赖于谱半径ρ(B)ρ(B)（而ρ(B)ρ(B)又依赖于A），因此**雅可比方法并非对于所有的A都能收敛，甚至并非所有的正定阵A都不一定收敛。**
